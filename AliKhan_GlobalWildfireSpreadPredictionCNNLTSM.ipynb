{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hF6UFOPUb6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc11e191-9fe9-413e-d2a1-24ced79e5b52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!pip install h5py\n",
        "!pip install h5pyViewer"
      ],
      "metadata": {
        "id": "cRloC7ioUmK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6d5650c-c0b1-49f5-e135-09c2151ff3cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.6.15)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting h5pyViewer\n",
            "  Downloading h5pyViewer-0.0.1.6.tar.gz (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 2.9 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/ed/cf/dee51494ef114bdbed66ee80743fb0ab44eb9444cb93478a41241a730208/h5pyViewer-0.0.1.6.tar.gz#sha256=6553c4af832fd2d8e7b5184ca650bd603b0502fd459f2cce0ffeda0de2bc1fc2 (from https://pypi.org/simple/h5pyviewer/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading h5pyViewer-0.0.1.5.tar.gz (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 2.9 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/f6/3f/4e01cd744b79c49155ac5f3ad21f30f5a7a758d0bbc15bce10fdea02d47b/h5pyViewer-0.0.1.5.tar.gz#sha256=701c5e36d16d2e9a20382550fceb7b06f775308a637cb868f9a74e8c440260bf (from https://pypi.org/simple/h5pyviewer/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading h5pyViewer-0.0.1.4.tar.gz (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.1 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/c7/15/88bb4d1f8b617cee553190267029feebdba5b9d3865674862158b22308f3/h5pyViewer-0.0.1.4.tar.gz#sha256=10193e6cd7d226e62be084084783c1b088dacc65d309a6bc60444d6450bb362a (from https://pypi.org/simple/h5pyviewer/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading h5pyViewer-0.0.1.3.tar.gz (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.0 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/47/90/41824de4b882a7c00a28012914bccc4d8cc8e0840bc3a1ad0a62a49c9602/h5pyViewer-0.0.1.3.tar.gz#sha256=4d8b0cf4a76897365d0109ed73f71c997a45d199e5d9734db4f91aa0667804b7 (from https://pypi.org/simple/h5pyviewer/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading h5pyViewer-0.0.1.2.tar.gz (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 2.7 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/15/18/9e55b10814e80cea025af04cecdda40d4ca6d0b92a4081aaf449bc319ba1/h5pyViewer-0.0.1.2.tar.gz#sha256=5bc9d42d108933f3fef21f9d15ed76f339bdb181da760c3edca0ad58e5e3a57f (from https://pypi.org/simple/h5pyviewer/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading h5pyViewer-0.0.1.1.tar.gz (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.8 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/44/ad/f348c71e8b56128715f2632b70c8f628fb14640f5da9b7666ecd6d811423/h5pyViewer-0.0.1.1.tar.gz#sha256=f2e3aa8ada9dec1e572686aeab67d7afea5772e82f4c10e127681669f6cc9481 (from https://pypi.org/simple/h5pyviewer/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading h5pyViewer-0.0.1.0.tar.gz (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 2.3 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/29/e6/542c8d6f578e9993c7981388b86c7d8b856c139aaf179ef10fdf850da53d/h5pyViewer-0.0.1.0.tar.gz#sha256=55f24381610a6877483027df4a9d7bbfdcf09cf9412f55c6812b8daae3fbb66f (from https://pypi.org/simple/h5pyviewer/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading h5pyViewer-0.0.0.33.tar.gz (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 2.3 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/0b/48/e225e3b2f3b7207e845a47da43df2f436de7abcfd9f312135b63c53dd7e5/h5pyViewer-0.0.0.33.tar.gz#sha256=2c36f7625ef8320a73976665656dc6c8f638c436fd16bee1ebf378fc08d76f4a (from https://pypi.org/simple/h5pyviewer/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading h5pyViewer-0.0.0.32.tar.gz (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 1.7 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/20/e9/3c8fa1c38aff1d660d4dc0849f1c70d5afb2a4723236413b0043918e9ca7/h5pyViewer-0.0.0.32.tar.gz#sha256=50f7371623f7a28b4a0d8e100ca8b9ec9f7cea33409332fdbb40641da3061e47 (from https://pypi.org/simple/h5pyviewer/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading h5pyViewer-0.0.0.30.tar.gz (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.2 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/78/79/08e57c424e26bc38a5d3bf2681b22776e95f10ceeca7e059b4f39f355977/h5pyViewer-0.0.0.30.tar.gz#sha256=8997c83d25c03e59a60f37e39262ab2d5cfddf05a16c137555c20c96f0cc0357 (from https://pypi.org/simple/h5pyviewer/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading h5pyViewer-0.0.0.28.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 3.9 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/14/2d/dc9195ab3211e45007881cf8c7d22f237be31822fbfbe02f37b529d5388a/h5pyViewer-0.0.0.28.tar.gz#sha256=fd22ae1382ade4eeb84da9e1eedb1715a3432ff269865cbb17a93a30dfc6d098 (from https://pypi.org/simple/h5pyviewer/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading h5pyViewer-0.0.0.27.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 6.5 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/94/26/0bd5cc5a23404547f5f5c1313ff7f152cb2feec8c6f8c805a72ffaba390f/h5pyViewer-0.0.0.27.tar.gz#sha256=62ef3f63c8c3b95799aefe1692264fa0c2e8ff67feb1ac9bd898029f07fd6a8f (from https://pypi.org/simple/h5pyviewer/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading h5pyViewer-0.0.0.26.tar.gz (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.4 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/84/ff/5477c649d849e9ca385037630a47da3632db7d4538b7b2acc197a6101803/h5pyViewer-0.0.0.26.tar.gz#sha256=8ce8b9d719c39f9b6f77d2e4ddebc21dcbe989cfec333b228bf0d5e6c68bc2b0 (from https://pypi.org/simple/h5pyviewer/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading h5pyViewer-0.0.0.25.tar.gz (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 5.0 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/23/86/3d6e6db41a1b810b64ec93b81f63342a61fb54a4d2894249ec7c714bc5c8/h5pyViewer-0.0.0.25.tar.gz#sha256=a9c762c3e94911d26d552c8882f162d2040ab481d1796f0fb7f85d49d67a514f (from https://pypi.org/simple/h5pyviewer/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading h5pyViewer-0.0.0.24.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 5.6 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/ff/56/6b38424a467f39024e30fccd51a29c70db2481670c0300e9ef57a419404a/h5pyViewer-0.0.0.24.tar.gz#sha256=1e534015947e92131d39aaa41a662a8e7d1dbf26151a184a00272047e43d8eef (from https://pypi.org/simple/h5pyviewer/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[?25h  Downloading h5pyViewer-0.0.0.19.tar.gz (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 4.6 MB/s \n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/60/00/6069823ea3ed25a3df96158c1422bb18dbf3f38211994c38f49aac0eaca1/h5pyViewer-0.0.0.19.tar.gz#sha256=c714c880a717ee72e34740edda7718f7f87dc83ae52871be4b3ac072543ec3d5 (from https://pypi.org/simple/h5pyviewer/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement h5pyViewer (from versions: 0.0.0.19, 0.0.0.24, 0.0.0.25, 0.0.0.26, 0.0.0.27, 0.0.0.28, 0.0.0.30, 0.0.0.32, 0.0.0.33, 0.0.1.0, 0.0.1.1, 0.0.1.2, 0.0.1.3, 0.0.1.4, 0.0.1.5, 0.0.1.6)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for h5pyViewer\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle/ && cp /content/drive/MyDrive/kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "ZWA1SnUXUnPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d fantineh/next-day-wildfire-spread"
      ],
      "metadata": {
        "id": "iEGEPI0kU5X2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eb3e828-b645-4af0-dd85-d1f9f4cb0332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading next-day-wildfire-spread.zip to /content\n",
            " 99% 2.06G/2.08G [00:09<00:00, 189MB/s]\n",
            "100% 2.08G/2.08G [00:09<00:00, 237MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/next-day-wildfire-spread.zip"
      ],
      "metadata": {
        "id": "YM3o9WdoU7HE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "427f3f3a-15ad-4e95-8439-607cfce64df1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/next-day-wildfire-spread.zip\n",
            "  inflating: next_day_wildfire_spread_eval_00.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_eval_01.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_test_00.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_test_01.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_00.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_01.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_02.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_03.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_04.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_05.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_06.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_07.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_08.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_09.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_10.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_11.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_12.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_13.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_14.tfrecord  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import Dict, List, Optional, Text, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import io\n",
        "import imageio\n",
        "from IPython.display import Image, display\n",
        "from ipywidgets import widgets, Layout, HBox\n",
        "import tensorflow_datasets as tfds\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "Jg__CKnlU7nA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_pattern = '/content/next_day_wildfire_spread_train*'"
      ],
      "metadata": {
        "id": "UxsILo-2U953"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_FEATURES = ['elevation', 'th', 'vs',  'tmmn', 'tmmx', 'sph',\n",
        "                  'pr', 'pdsi', 'NDVI', 'population', 'erc', 'PrevFireMask']\n",
        "\n",
        "OUTPUT_FEATURES = ['FireMask', ]\n",
        "\n",
        "DATA_STATS = {'elevation': (0.0, 3141.0, 657.3003, 649.0147), 'pdsi': (-6.1298, 7.8760, -0.0053, 2.6823), 'NDVI': (-9821.0, 9996.0, 5157.625, 2466.6677), 'pr': (0.0, 44.5304, 1.7398051, 4.4828), 'sph': (0., 1., 0.0071658953, 0.0042835088), 'th': (0., 360.0, 190.3298, 72.5985), 'tmmn': (253.15, 298.9489, 281.08768, 8.9824), 'tmmx': (253.15, 315.0923, 295.17383, 9.8155), 'vs': (0.0, 10.0243, 3.8501, 1.4110), 'erc': (0.0, 106.2489, 37.3263, 20.8460), 'population': (0., 2534.0630, 25.5314, 154.7233), 'PrevFireMask': (-1., 1., 0., 1.), 'FireMask': (-1., 1., 0., 1.)} \n"
      ],
      "metadata": {
        "id": "VYycWzWCVBnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import io\n",
        "import imageio\n",
        "from IPython.display import Image, display\n",
        "from ipywidgets import widgets, Layout, HBox\n"
      ],
      "metadata": {
        "id": "iBVXb4sWVvPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def input_output_images(input_img:tf.Tensor, output_img:tf.Tensor, sample_size:int, num_in_channels:int, num_out_channels:int):\n",
        "  combined = tf.concat([input_img, output_img], axis=2)\n",
        "  combined = tf.image.random_crop(combined,[sample_size, sample_size, num_in_channels + num_out_channels])\n",
        "  input_img = combined[:, :, 0:num_in_channels]\n",
        "  output_img = combined[:, :, -num_out_channels:]\n",
        "  return input_img, output_img\n",
        "  #return combined\n",
        "#input_output_images(input_img, output_img, sample_size, num_in_channels, 1)\n"
      ],
      "metadata": {
        "id": "DU9yFmT9Wa_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def random_crop_input_and_output_images(input_img:tf.Tensor, output_img:tf.Tensor, sample_size:int, num_in_channels:int, num_out_channels:int,) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  combined = tf.concat([input_img, output_img], axis=2)\n",
        "  combined = tf.image.random_crop(\n",
        "      combined,\n",
        "      [sample_size, sample_size, num_in_channels + num_out_channels])\n",
        "  input_img = combined[:, :, 0:num_in_channels]\n",
        "  output_img = combined[:, :, -num_out_channels:]\n",
        "  return input_img, output_img\n",
        "\n",
        "\n",
        "def center_crop_input_and_output_images(input_img: tf.Tensor, output_img: tf.Tensor, sample_size: int,\n",
        ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  central_fraction = sample_size / input_img.shape[0]\n",
        "  input_img = tf.image.central_crop(input_img, central_fraction)\n",
        "  output_img = tf.image.central_crop(output_img, central_fraction)\n",
        "  return input_img, output_img"
      ],
      "metadata": {
        "id": "_BysQjFZb18t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def _get_base_key(key: Text) -> Text:\n",
        "  match = re.match(r'[a-zA-Z]+', key)\n",
        "  if match:\n",
        "    return match.group(1)\n",
        "  raise ValueError(\n",
        "      f'The provided key does not match the expected pattern: {key}')\n",
        "\n",
        "\n",
        "def _clip_and_rescale(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n",
        "  base_key = _get_base_key(key)\n",
        "  if base_key not in DATA_STATS:\n",
        "    raise ValueError(\n",
        "        'No data statistics available for the requested key: {}.'.format(key))\n",
        "  min_val, max_val, _, _ = DATA_STATS[base_key]\n",
        "  inputs = tf.clip_by_value(inputs, min_val, max_val)\n",
        "  return tf.math.divide_no_nan((inputs - min_val), (max_val - min_val))\n",
        "\n",
        "\n",
        "def _clip_and_normalize(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n",
        "  base_key = _get_base_key(key)\n",
        "  if base_key not in DATA_STATS:\n",
        "    raise ValueError(\n",
        "        'No data statistics available for the requested key: {}.'.format(key))\n",
        "  min_val, max_val, mean, std = DATA_STATS[base_key]\n",
        "  inputs = tf.clip_by_value(inputs, min_val, max_val)\n",
        "  inputs = inputs - mean\n",
        "  return tf.math.divide_no_nan(inputs, std)\n",
        "\n",
        "def _get_features_dict(\n",
        "    sample_size: int,\n",
        "    features: List[Text],\n",
        ") -> Dict[Text, tf.io.FixedLenFeature]:\n",
        "  sample_shape = [sample_size, sample_size]\n",
        "  features = set(features)\n",
        "  columns = [\n",
        "      tf.io.FixedLenFeature(shape=sample_shape, dtype=tf.float32)\n",
        "      for _ in features\n",
        "  ]\n",
        "  return dict(zip(features, columns))\n",
        "\n",
        "\n",
        "def _parse_fn(example_proto: tf.train.Example, data_size: int, sample_size: int, num_in_channels: int, clip_and_normalize: bool, clip_and_rescale: bool, random_crop: bool, center_crop: bool,) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  if (random_crop and center_crop):\n",
        "    raise ValueError('Cannot have both random_crop and center_crop be True')\n",
        "  input_features, output_features = INPUT_FEATURES, OUTPUT_FEATURES\n",
        "  feature_names = input_features + output_features\n",
        "  features_dict = _get_features_dict(data_size, feature_names)\n",
        "  features = tf.io.parse_single_example(example_proto, features_dict)\n",
        "\n",
        "  if clip_and_normalize:\n",
        "    inputs_list = [_clip_and_normalize(features.get(key), key) for key in input_features]\n",
        "  elif clip_and_rescale:\n",
        "    inputs_list = [_clip_and_rescale(features.get(key), key) for key in input_features]\n",
        "  else:\n",
        "    inputs_list = [features.get(key) for key in input_features]\n",
        "\n",
        "  inputs_stacked = tf.stack(inputs_list, axis=0)\n",
        "  input_img = tf.transpose(inputs_stacked, [1, 2, 0])\n",
        "\n",
        "  outputs_list = [features.get(key) for key in output_features]\n",
        "  assert outputs_list, 'outputs_list should not be empty'\n",
        "  outputs_stacked = tf.stack(outputs_list, axis=0)\n",
        "\n",
        "  outputs_stacked_shape = outputs_stacked.get_shape().as_list()\n",
        "  assert len(outputs_stacked.shape) == 3, ('outputs_stacked should be rank 3''but dimensions of outputs_stacked'f' are {outputs_stacked_shape}')\n",
        "  output_img = tf.transpose(outputs_stacked, [1, 2, 0])\n",
        "\n",
        "  if random_crop:\n",
        "    input_img, output_img = random_crop_input_and_output_images(input_img, output_img, sample_size, num_in_channels, 1)\n",
        "  if center_crop:\n",
        "    input_img, output_img = center_crop_input_and_output_images(input_img, output_img, sample_size)\n",
        "  \n",
        "  # insert empty dimension to make it work with ConvLSTM\n",
        "  input_img = tf.expand_dims(input_img, axis=0)  \n",
        "  output_img = tf.expand_dims(output_img, axis=0)\n",
        "\n",
        "  return input_img, output_img\n",
        "\n",
        "\n",
        "def get_dataset(file_pattern: Text, data_size: int, sample_size: int, batch_size: int, num_in_channels: int, compression_type: Text, clip_and_normalize: bool, clip_and_rescale: bool, random_crop: bool, center_crop: bool) -> tf.data.Dataset:\n",
        "  if (clip_and_normalize and clip_and_rescale):\n",
        "    raise ValueError('Cannot have both normalize and rescale.')\n",
        "  dataset = tf.data.Dataset.list_files(file_pattern)\n",
        "  dataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x, compression_type=compression_type), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.map(lambda x: _parse_fn(x, data_size, sample_size, num_in_channels, clip_and_normalize, clip_and_rescale, random_crop, center_crop),num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "MSMBb9r1bcLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = get_dataset(\n",
        "      file_pattern,\n",
        "      data_size=64,\n",
        "      sample_size=32,\n",
        "      batch_size=100,\n",
        "      num_in_channels=12,\n",
        "      compression_type=None,\n",
        "      clip_and_normalize=False,\n",
        "      clip_and_rescale=False,\n",
        "      random_crop=True,\n",
        "      center_crop=False)"
      ],
      "metadata": {
        "id": "13_rX0Z8c3BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = get_dataset(\n",
        "      '/content/next_day_wildfire_spread_eval*',\n",
        "      data_size=64,\n",
        "      sample_size=32,\n",
        "      batch_size=100,\n",
        "      num_in_channels=12,\n",
        "      compression_type=None,\n",
        "      clip_and_normalize=False,\n",
        "      clip_and_rescale=False,\n",
        "      random_crop=True,\n",
        "      center_crop=False)"
      ],
      "metadata": {
        "id": "vlyV-JINf7Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Basic Info\n",
        "print(f\"Size of training dataset: {len(list(dataset)) * 100}\")\n",
        "print(dataset)\n",
        "print(f\"\\nSize of validation datset: {len(list(val_dataset)) * 100}\")\n",
        "print(val_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrrJlP5jgLLj",
        "outputId": "b821c5f5-feb1-40e8-e190-e76ffa3b27be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of training dataset: 15000\n",
            "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 1, 32, 32, 12), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 32, 32, 1), dtype=tf.float32, name=None))>\n",
            "\n",
            "Size of validation datset: 1900\n",
            "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 1, 32, 32, 12), dtype=tf.float32, name=None), TensorSpec(shape=(None, 1, 32, 32, 1), dtype=tf.float32, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# U-NET model\n"
      ],
      "metadata": {
        "id": "VwnfkVZp4MP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the input layer with no definite frame size.\n",
        "inp = layers.Input(shape=(1, 32, 32, 12))\n",
        "\n",
        "# We will construct 3 `ConvLSTM2D` layers with batch normalization,\n",
        "# followed by a `Conv3D` layer for the spatiotemporal outputs.\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=32,\n",
        "    kernel_size=(5, 5),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(inp)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=32,\n",
        "    kernel_size=(3, 3),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=32,\n",
        "    kernel_size=(1, 1),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(x)\n",
        "x = layers.Conv3D(\n",
        "    filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n",
        ")(x)\n",
        "\n",
        "# Next, we will build the complete model and compile it.\n",
        "model = keras.models.Model(inp, x)\n",
        "model.compile(\n",
        "    loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(),\n",
        "    metrics=[tf.keras.metrics.AUC(curve='PR')]\n",
        ")\n"
      ],
      "metadata": {
        "id": "LasGGLjaQhjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Construct the input layer with no definite frame size.\n",
        "# inp = layers.Input(shape=(32, 32, 12))\n",
        "\n",
        "# # We will construct 3 `ConvLSTM2D` layers with batch normalization,\n",
        "# # followed by a `Conv3D` layer for the spatiotemporal outputs.\n",
        "# x = layers.Conv2D(\n",
        "#     filters=32, \n",
        "#     kernel_size=(3,3),\n",
        "#     padding=\"same\",\n",
        "#     activation=\"relu\",\n",
        "# )(inp)\n",
        "# x = layers.BatchNormalization()(x)\n",
        "# x = layers.MaxPooling2D(\n",
        "#     pool_size=(2,2),\n",
        "# )(inp)\n",
        "# x = layers.Conv2D(\n",
        "#     filters=32,\n",
        "#     kernel_size=(3, 3),\n",
        "#     padding=\"same\",\n",
        "#     activation=\"relu\",\n",
        "# )(inp)\n",
        "# x = layers.BatchNormalization()(x)\n",
        "# x = layers.Conv2D(\n",
        "#     filters=1, kernel_size=(3, 3), activation=\"sigmoid\", padding=\"same\"\n",
        "# )(x)\n",
        "\n",
        "# # Next, we will build the complete model and compile it.\n",
        "# model = keras.models.Model(inp, x)\n",
        "# model.compile(\n",
        "#     loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(learning_rate=0.001,),\n",
        "#     metrics=[tf.keras.metrics.AUC(curve='PR')]\n",
        "# )"
      ],
      "metadata": {
        "id": "Yx1eVdno-58c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "class TimingCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, logs={}):\n",
        "        self.logs=[]\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.starttime = timer()\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.logs.append(timer()-self.starttime)\n",
        "\n",
        "tcb = TimingCallback()"
      ],
      "metadata": {
        "id": "USgjUXr8kWRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint = ModelCheckpoint(\"model.hdf5\", monitor = 'val_auc', verbose = 1, save_best_only = False, period = 1)\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5)\n",
        "\n",
        "# Define modifiable training hyperparameters.\n",
        "epochs = 5\n",
        "batch_size = 100\n",
        "\n",
        "total_time_to_train = timer()\n",
        "# Fit the model to the training data.\n",
        "history = model.fit(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks= [early_stopping, reduce_lr, tcb] # , checkpoint]\n",
        ")\n",
        "total_time_to_train = timer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAO5cA0sUMaX",
        "outputId": "c1644b03-ee13-483b-ee8c-22056954dec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "150/150 [==============================] - 8s 49ms/step - loss: 0.2551 - auc_3: 0.0474 - val_loss: 0.0087 - val_auc_3: 0.0529\n",
            "Epoch 2/5\n",
            "150/150 [==============================] - 6s 42ms/step - loss: -0.0314 - auc_3: 0.0501 - val_loss: -0.0569 - val_auc_3: 0.0556\n",
            "Epoch 3/5\n",
            "150/150 [==============================] - 6s 43ms/step - loss: -0.1233 - auc_3: 0.0521 - val_loss: -0.0822 - val_auc_3: 0.0563\n",
            "Epoch 4/5\n",
            "150/150 [==============================] - 6s 43ms/step - loss: -0.2415 - auc_3: 0.0532 - val_loss: -0.2932 - val_auc_3: 0.0541\n",
            "Epoch 5/5\n",
            "150/150 [==============================] - 7s 43ms/step - loss: -0.3977 - auc_3: 0.0532 - val_loss: 0.1218 - val_auc_3: 0.0532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average time per epoch: {sum(tcb.logs)/len(tcb.logs)} seconds\")\n",
        "print(f\"Total Time taken: {sum(tcb.logs)} seconds\")\n",
        "print(f\"Total time to train: {total_time_to_train} seconds\")"
      ],
      "metadata": {
        "id": "J8rJW0GKkbeY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ea06ad1-cb15-4a23-d588-b54a8b889bd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average time per epoch: 23.479602731799986 seconds\n",
            "Total Time taken: 117.39801365899993 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# history.history"
      ],
      "metadata": {
        "id": "SSTKvANqlaGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"model\")"
      ],
      "metadata": {
        "id": "pq6qAUhfdsZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(149):\n",
        "#   features, firemask = next(iter(dataset))\n",
        "#   features_np = features.numpy()\n",
        "#   firemask_np = firemask.numpy()\n",
        "#   features_np = np.expand_dims(features_np, axis = 1)\n",
        "#   features_np.shape\n",
        "#   firemask_np = np.expand_dims(firemask_np, axis = 1)\n",
        "#   firemask_np.shape  \n",
        "#   algo_train()\n",
        "  "
      ],
      "metadata": {
        "id": "n2eXswj7O9yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#with h5py.File('model.hdf5', 'r') as f:\n",
        "    #data = f['']\n",
        "    #data = f['val_acc']\n",
        "    # get the minimum value\n",
        "    #ls = list(f.keys())\n",
        "    #print(ls)\n",
        "    #data = f.get('val_loss')\n",
        "    #dataset1 = np.array(data)\n",
        "    #print(dataset1)"
      ],
      "metadata": {
        "id": "GuLkmetbUnCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "hu_Z9PtKYFKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43884c12-da41-4a85-cf90-44e425d8dd94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1, 32, 32, 12)]   0         \n",
            "                                                                 \n",
            " conv_lstm2d (ConvLSTM2D)    (None, 1, 32, 32, 32)     140928    \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 1, 32, 32, 32)    128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv_lstm2d_1 (ConvLSTM2D)  (None, 1, 32, 32, 32)     73856     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 1, 32, 32, 32)    128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv_lstm2d_2 (ConvLSTM2D)  (None, 1, 32, 32, 32)     8320      \n",
            "                                                                 \n",
            " conv3d (Conv3D)             (None, 1, 32, 32, 1)      865       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 224,225\n",
            "Trainable params: 224,097\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils.vis_utils import plot_model\n",
        "#model = Sequential()\n",
        "#model.add(Dense(2, input_dim=1, activation='relu'))\n",
        "#model.add(Dense(1, activation='sigmoid'))\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True, rankdir = 'TB', expand_nested = True)\n"
      ],
      "metadata": {
        "id": "I7ZK4zsHQf2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Viz"
      ],
      "metadata": {
        "id": "C6OzppsJyhja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "t8nuz5nlxt7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CMAP = colors.ListedColormap(['black', 'silver', 'orangered'])\n",
        "BOUNDS = [-1, -0.1, 0.001, 1]\n",
        "NORM = colors.BoundaryNorm(BOUNDS, CMAP.N)\n",
        "SAMPLE = 1  # row\n",
        "FIREMASK_IDX = 11  # the index of the feature with the firemask at t"
      ],
      "metadata": {
        "id": "DNrFE85SygPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_pred = np.zeros(shape=(32,32))\n",
        "for i in range(32):\n",
        "  for j in range(32):\n",
        "    x_pred[i,j] = features_val_np[SAMPLE][0][i,j][FIREMASK_IDX]\n",
        "# plt.imshow(recolor(x_pred))\n",
        "x_pred = features_val_np[SAMPLE, 0, :, :, FIREMASK_IDX]\n",
        "plt.imshow(x_pred, cmap=CMAP, norm=NORM)\n",
        "# print(recolor(x_pred).ravel()[0:1024].reshape((32,32)))\n",
        "plt.title(\"FIRE MASK T\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ojx622wGd0tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = firemask_val_np[SAMPLE].reshape([32,32])\n",
        "# plt.imshow(recolor(y_true))\n",
        "plt.imshow(y_true, cmap=CMAP, norm=NORM)\n",
        "plt.title(\"FIRE MASK T+1 Ground Truth\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zelccY5Rd-Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.expand_dims(features_val_np[SAMPLE],axis=0)\n",
        "y_pred = model.predict(x)\n",
        "y_pred_reshaped = y_pred.ravel().reshape((32,32))\n",
        "\n",
        "CMAP = colors.ListedColormap(['silver', 'orangered'])\n",
        "BOUNDS = [0.0, 0.25, 1]\n",
        "NORM = colors.BoundaryNorm(BOUNDS, CMAP.N)\n",
        "\n",
        "plt.imshow(y_pred_reshaped,cmap=CMAP, norm=NORM)\n",
        "# plt.imshow(y_pred_reshaped)\n",
        "plt.colorbar()\n",
        "plt.title(\"FIRE MASK T+1 Predicted\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v3pmLsAreADU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1,3)\n",
        "fig.suptitle('Firemasks')\n",
        "axs[0].imshow(x_pred)\n",
        "axs[0].set_title(\"Input\")\n",
        "axs[1].imshow(y_true)\n",
        "axs[1].set_title(\"Ground Truth\")\n",
        "axs[2].imshow(y_pred_reshaped)\n",
        "axs[2].set_title(\"Predicted\")\n",
        "fig.colorbar()\n",
        "fig.axis(\"off\")"
      ],
      "metadata": {
        "id": "K4xwkPDneCQ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}